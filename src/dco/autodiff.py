from typing import Callable

import jax
import numpy as np
from numpy.typing import NDArray


def nabla(
    f: Callable[[NDArray[np.float64]], float | NDArray[np.float64] | jax.Array],
    use_jax: bool,
) -> Callable[[NDArray[np.float64]], NDArray[np.float64]]:
    """
    Returns a gradient function for the given function f using either JAX or Autograd.

    Parameters
    ----------
    f : Callable[[NDArray[float64]], float64 | Array]
        The function for which to compute the gradient.

    use_jax : bool
        Whether to use JAX for automatic differentiation.
        If False, Autograd will be used, which requires the 'autograd' package to be installed.
        Use 'pip install discoopt[autograd]' to install the package.
        This option is useful when JAX does not work properly (e.g., on Raspberry Pi).

    Returns
    -------
    Callable[[NDArray[float64]], NDArray[float64]]
        A function that computes the gradient of f.
    """

    if not use_jax:
        import autograd

        return autograd.grad(f)  # type: ignore

    jax.config.update("jax_platforms", "cpu")

    raw_grad = jax.jit(jax.grad(f))

    def wrapped_grad(x: NDArray[np.float64]) -> NDArray[np.float64]:
        grad_val = jax.device_get(raw_grad(x))
        return np.asarray(grad_val).astype(np.float64, copy=False)

    return wrapped_grad
